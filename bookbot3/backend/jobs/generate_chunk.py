"""
Text generation job for BookBot.

This job handles generating, rewriting, editing, copyediting, and reviewing
text content for scene chunks using configured bots.
"""

import time
import json
from typing import Dict, Any, Optional # Ensure all are imported

# import yaml # yaml is imported but not used, can be removed later if confirmed.
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.ext.mutable import MutableDict

from backend.models import utcnow # Corrected import path
from backend.models import Chunk, Job, JobLog, Book, db
from backend.jobs.generate_chunk_utils import resolve_template_variables
from backend.jobs.template_resolver import build_placeholder_values
from backend.llm import LLMCall # This is the intended LLMCall (e.g., FakeLLMCall)

# Local stub for specific testing scenarios if needed, distinct from the main LLMCall.
class LLMCallStub:
    def __init__(self, prompt, **kwargs):
        self.prompt = prompt
        self.kwargs = kwargs
        self.bot_chunk = kwargs.get('bot_chunk')

        self.model = kwargs.get('model', 'stub-default-model')
        self.api_key = kwargs.get('api_key', 'stub-no-key')
        self.system_prompt = self.bot_chunk.text if self.bot_chunk else 'Default system prompt for stub.'
        self.target_word_count = kwargs.get('target_word_count', 100)
        self.llm_params = kwargs.get('llm_params', {})
        
        # Simulate attributes that the main LLMCall would have
        self.input_tokens = len(prompt.split()) + len(self.system_prompt.split()) 
        self.output_text = ""
        self.cost = 0.0
        self.output_tokens = 0
        self.stop_reason = "stub_stop"
        self.error_message = None


    def execute(self):
        mode = self.kwargs.get('mode', 'write')
        bot_name = "Unknown Bot"
        if self.bot_chunk:
            bot_props = {}
            if isinstance(self.bot_chunk.props, dict):
                bot_props = self.bot_chunk.props
            elif isinstance(self.bot_chunk.props, str):
                try:
                    bot_props = json.loads(self.bot_chunk.props)
                except (json.JSONDecodeError, TypeError):
                    pass 
            bot_name = bot_props.get('title', 'Unknown Bot')

        output_intro = (
            f"[Test Generation Result - Model: {self.model}]\n"
            f"Mode: {mode}\n"
            f"Generated by {bot_name} (using API Key: {'present' if self.api_key != 'stub-no-key' else 'not present'})\n"
            f"System Prompt Snippet: {self.system_prompt[:70]}...\n"
            f"Target words: {self.target_word_count}\n"
            f"Other LLM Params: {json.dumps(self.llm_params)}\n"
            f"Generated at: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        )
        content_section = ""
        if mode != 'write' and self.prompt:
            content_section = f"\n\nOriginal content (approx. {len(self.prompt.split())} words): {self.prompt[:200]}...\n"
        generated_body = f"This is test content generated by the stubbed LLM for mode '{mode}'.\nOriginal prompt length: {len(self.prompt)} characters."
        self.output_text = output_intro + generated_body + content_section
        self.output_tokens = len(self.output_text.split())
        self.cost = round(self.output_tokens * 0.000005, 6) # Simplified cost
        return True


class GenerateChunkJob:
    """Job for generating text content for chunks using AI."""
    
    # The allowed LLM group for this job type
    allowed_llm_group = "writer"

    def __init__(self, *args):
        if len(args) == 1 and isinstance(args[0], Job): # Check if it's a Job instance
            self.job = args[0]
        elif len(args) == 3: # job_id, props, app_context (app_context might not be needed if db is configured)
            job_id, props, _ = args 
            class SimpleJob: # Minimal Job-like object for cases where a full Job instance isn't passed
                def __init__(self, job_id, props):
                    self.job_id = job_id
                    self.props = props
                    self.book_id = props.get('book_id')
                    self.job_type = 'GenerateChunk' # Should match actual job type
                    self.state = 'pending'
            self.job = SimpleJob(job_id, props)
        else:
            raise TypeError(f"Invalid constructor args for GenerateChunkJob: {args}")

        self.props = getattr(self.job, 'props', {}) or {}
        self.chunk_id = self.props.get('chunk_id') # Convenience
        self.bot_id = self.props.get('bot_id')     # Convenience
        self.mode = self.props.get('mode')         # Convenience

        if hasattr(self.job, 'book_id') and self.job.book_id:
            # Assuming db session is available and configured
            self.book = db.session.get(Book, self.job.book_id) if db.session else None
        else:
            self.book = None

    def log(self, message: str, level: str = 'INFO', props_data: Optional[Dict[str, Any]] = None):
        """Log a message for this job."""
        # print(f"[{self.job.job_type}:{self.job.job_id[:8]}] {level}: {message}") # Keep console log for immediate debugging

        log_entry_args = {
            'job_id': self.job.job_id,
            'log_entry': message,
            'log_level': level
        }
        if props_data:
            log_entry_args['props'] = props_data # JobLog.props is a JSON field
        
        try:
            log_entry = JobLog(**log_entry_args)
            db.session.add(log_entry)
            db.session.commit()
        except Exception as e:
            print(f"Failed to write log to DB: {e}") # Fallback log

    def is_cancelled(self) -> bool:
        """Check if the job has been cancelled."""
        if not hasattr(self.job, 'state'): return False # SimpleJob might not have state refreshed
        try:
            db.session.refresh(self.job)
            return self.job.state == 'cancelled'
        except Exception as e:
            print(f"Could not refresh job state: {e}")
            return False


    def _get_props(self, chunk: Chunk) -> Dict[str, Any]:
        """Get chunk props as a dictionary, handling both dict and JSON string formats."""
        if isinstance(chunk.props, dict):
            return chunk.props
        elif isinstance(chunk.props, str):
            try:
                return json.loads(chunk.props)
            except (json.JSONDecodeError, TypeError):
                self.log(f"Warning: Could not parse props for chunk {chunk.chunk_id}", level='WARNING')
                return {}
        return {} 

    def validate(self) -> bool:
        """Validate the job parameters."""
        if not self.props.get('chunk_id'):
            self.log("Error: chunk_id is required in job props.", level='ERROR')
            return False
        if self.props.get('mode') not in ['write', 'rewrite', 'edit', 'copyedit', 'review']:
            self.log(f"Error: Invalid mode '{self.props.get('mode')}' in job props.", level='ERROR')
            return False
        if not self.props.get('bot_id'):
            self.log("Error: bot_id is required in job props.", level='ERROR')
            return False
        return True

    def execute(self) -> bool:
        """Execute the text generation job."""
        active_chunk_id = self.props.get('chunk_id')

        if not hasattr(self.job, 'state'):
            self.log("Job object lacks 'state' attribute. Cannot proceed.", level='CRITICAL')
            return False

        try:
            if not self.validate():
                self.job.state = 'failed'
                db.session.commit()
                return False

            self.log(f"Starting text generation for chunk {active_chunk_id}")
            self.log(f"Mode: {self.props.get('mode')}, Bot: {self.props.get('bot_id')}")

            chunk = Chunk.query.filter_by(chunk_id=active_chunk_id).first()
            if not chunk:
                self.log(f"Error: Chunk {active_chunk_id} not found.", level='ERROR')
                self.job.state = 'failed'
                db.session.commit()
                return False

            self.log(f"Locking chunk {active_chunk_id} for generation.")
            chunk.is_locked = True
            db.session.commit()

            bot_chunk = Chunk.query.filter_by(chunk_id=self.props.get('bot_id')).first()
            if not bot_chunk or bot_chunk.type != 'bot':
                self.log(f"Error: Bot {self.props.get('bot_id')} not found or invalid.", level='ERROR')
                chunk.is_locked = False
                self.job.state = 'failed'
                db.session.commit()
                return False

            self.log("Preparing and logging LLM call parameters...")
            chunk_specific_props = self._get_props(chunk)
            bot_specific_props = self._get_props(bot_chunk)

            raw_model_identifier = bot_specific_props.get('model_name', 'default-claude-v3-haiku')
            model_name = "default-claude-v3-haiku"
            if isinstance(raw_model_identifier, str):
                model_name = raw_model_identifier
            elif hasattr(raw_model_identifier, 'model') and isinstance(getattr(raw_model_identifier, 'model'), str):
                model_name = getattr(raw_model_identifier, 'model')
                self.log(f"Warning: 'model_name' in bot_specific_props was an object. Extracted .model attribute: {model_name}", level='WARNING')
            elif hasattr(raw_model_identifier, 'model_name') and isinstance(getattr(raw_model_identifier, 'model_name'), str):
                model_name = getattr(raw_model_identifier, 'model_name')
                self.log(f"Warning: 'model_name' in bot_specific_props was an object. Extracted .model_name attribute: {model_name}", level='WARNING')
            else:
                self.log(f"Warning: 'model_name' in bot_specific_props was an unexpected type ({type(raw_model_identifier)}). Value: '{str(raw_model_identifier)[:100]}'. Using default: {model_name}", level='WARNING')

            api_key_to_use = bot_specific_props.get('api_key', 'not-set-in-bot-props')

            # The bot's `text` field contains the system prompt/template.
            system_prompt_template = bot_chunk.text or ""

            # Build placeholder values using the new resolver
            placeholder_values = build_placeholder_values(chunk, self.book)

            # Resolve template variables for the prompt
            prompt_template = bot_specific_props.get('prompt_template', '')
            if not prompt_template:
                self.log("No prompt template found in bot props, using chunk text as prompt.", level='WARNING')
                prompt = chunk.text
            else:
                prompt = resolve_template_variables(prompt_template, placeholder_values)

            # Resolve template variables for the system prompt
            system_prompt_text = resolve_template_variables(system_prompt_template, placeholder_values)
            
            target_words = chunk_specific_props.get('target_word_count', self.props.get('target_word_count', 250))
            other_llm_params = self.props.get('llm_params', {}).copy() # Start with user-defined params
            # Ensure default temperature if not provided by user, but prioritize user's setting
            if 'temperature' not in other_llm_params:
                other_llm_params['temperature'] = 0.7
            # Inject mode and bot_specific_props for FakeLLMCall or potentially other LLMs
            other_llm_params['mode'] = self.props.get('mode')
            other_llm_params['bot_specific_props'] = bot_specific_props

            prompt_text_for_llm = chunk.text or ""

            api_key_status_msg = "test-key" if api_key_to_use == "test-key" else ("not-set-in-bot-props" if api_key_to_use == "not-set-in-bot-props" else "API key present")
            llm_call_details_props = {
                "model_name": model_name,
                "api_key_status": api_key_status_msg,
                "system_prompt_snippet": (system_prompt_text[:100] + "...") if system_prompt_text else "",
                "prompt_snippet": (prompt_text_for_llm[:100] + "...") if prompt_text_for_llm else "",
                "target_word_count": target_words,
                "other_llm_params": other_llm_params
            }
            self.log(f"LLM Call Input: Model='{model_name}', TargetWords={target_words}", level='LLM', props_data=llm_call_details_props)

            llm = LLMCall(model=model_name, api_key=api_key_to_use, target_word_count=target_words, prompt=prompt_text_for_llm, system_prompt=system_prompt_text, llm_params=other_llm_params)
            self.log("Executing LLMCall...")
            llm_success = llm.execute()

            job_completed_with_placeholder = False

            if llm_success:
                llm_output_log_message = (f"LLM Output: Model='{llm.model}', InTokens={getattr(llm, 'input_tokens', 'N/A')}, OutTokens={getattr(llm, 'output_tokens', 'N/A')}, Cost=${getattr(llm, 'cost', 0.0):.6f}, StopReason='{getattr(llm, 'stop_reason', 'N/A')}'")
                llm_output_log_props = {
                    "llm_model": str(getattr(llm, 'model', 'N/A')),
                    "llm_input_tokens": int(getattr(llm, 'input_tokens', 0)),
                    "llm_output_tokens": int(getattr(llm, 'output_tokens', 0)),
                    "llm_cost": float(getattr(llm, 'cost', 0.0)),
                    "llm_stop_reason": str(getattr(llm, 'stop_reason', 'N/A')),
                    "llm_error_status": str(getattr(llm, 'error_status', '')) # Include error status if any
                }
                self.log(llm_output_log_message, level='LLM', props_data=llm_output_log_props)
                chunk.text = llm.output_text
                chunk.word_count = Chunk.count_words(llm.output_text)
                current_total_cost = self.job.props.get('total_llm_cost', 0.0)
                llm_call_cost = getattr(llm, 'cost', 0.0)
                self.job.props['total_llm_cost'] = current_total_cost + llm_call_cost
            else: # LLM call failed
                llm_output_log_props = {
                    "llm_model": str(getattr(llm, 'model', model_name)), # model_name is already a str
                    "llm_input_tokens": int(getattr(llm, 'input_tokens', 0)),
                    "llm_output_tokens": int(getattr(llm, 'output_tokens', 0)),
                    "llm_cost": float(getattr(llm, 'cost', 0.0)),
                    "llm_stop_reason": str(getattr(llm, 'stop_reason', 'N/A')),
                    "llm_error_status": str(getattr(llm, 'error_status', 'Unknown LLM Error'))
                }
                self.log(f"LLM call failed. Error: {str(getattr(llm, 'error_status', 'Unknown LLM Error'))}", level='ERROR', props_data=llm_output_log_props)
                placeholder_on_error = bot_specific_props.get('placeholder_on_error', True)
                if placeholder_on_error:
                    placeholder_text = bot_specific_props.get('placeholder_text', f"Placeholder text generated by {bot_specific_props.get('title', 'Test Bot')}.")
                    chunk.text = placeholder_text
                    chunk.word_count = Chunk.count_words(placeholder_text)
                    self.log(f"Used placeholder text due to LLM error: '{placeholder_text[:50]}...'", level='WARNING')
                    self.job.state = 'completed'
                    db.session.commit() # Commit placeholder text and state
                    job_completed_with_placeholder = True
                else:
                    self.job.state = 'failed'
                    db.session.commit() # Commit failed state
                    chunk.is_locked = False
                    db.session.commit()
                    return False # Job truly failed

            # Finalization for successful LLM or placeholder completion
            chunk.is_locked = False
            if self.job.state != 'completed': # Ensure state is completed if not already set by placeholder
                self.job.state = 'completed'
            self.job.completed_at = utcnow()
            self.log("Job completed successfully")
            db.session.commit()
            return True

        except Exception as e:
            exception_str = str(e)
            self.log(f"Critical error in GenerateChunkJob.execute: {exception_str}", level='CRITICAL', props_data={'exception': exception_str})
            if 'chunk' in locals() and chunk and hasattr(chunk, 'is_locked'):
                chunk.is_locked = False
            if hasattr(self.job, 'state'): # Check if job object is fully initialized
                self.job.state = 'failed'
            if hasattr(self.job, 'completed_at'):
                 self.job.completed_at = utcnow()
            try:
                db.session.commit()
            except Exception as commit_exc:
                self.log(f"Failed to commit state during critical error handling: {str(commit_exc)}", level='CRITICAL')
                db.session.rollback()
            return False

    def _generate_placeholder_text(self, chunk: Chunk, bot_chunk: Chunk, error_message: Optional[str] = None) -> str:
        """Generate placeholder text for testing or on failure."""
        bot_props = self._get_props(bot_chunk)
        bot_name = bot_props.get('title', bot_props.get('name', 'Unknown Bot'))
        llm_alias = bot_props.get('llm_alias', 'Unknown LLM (from bot props)')
        original_text = chunk.text or ""

        placeholder = f"""[Content Generation Attempt by {bot_name} (Bot ID: {bot_chunk.chunk_id}) using {llm_alias}]
Mode: {self.props.get('mode')}
Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
"""
        if error_message:
            placeholder += f"Status: FAILED. Error: {error_message}\n"
        else:
            placeholder += f"Status: Placeholder (LLM call may have been skipped or returned empty without explicit error).\n"

        placeholder += f"\nOriginal content length for chunk {chunk.chunk_id}: {len(original_text)} characters.\n"
        if len(original_text) > 0 :
             placeholder += f"Original content snippet: {original_text[:150]}...\n"
        
        placeholder += "\n---\nNo new content was generated or applied due to the issue noted above. The original text (if any) might still be present if the update failed before saving.\n---"
        return placeholder

    def run(self, callback=None):
        """Run method alias for execute (primarily for test compatibility)."""
        result = self.execute()
        if callback:
            callback()
        return result
